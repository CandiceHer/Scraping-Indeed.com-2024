{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09fc8bc3",
   "metadata": {},
   "source": [
    "This is the modified version of my submission for one of the assignments in UC Davis MSBA'24 - course 'Data Design & Representation'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c532d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import mysql.connector\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # turn off warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e4cd05",
   "metadata": {},
   "source": [
    "## Web Scraping Indeed.com\n",
    "1. Search target position ('data scientist' in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4995aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up wendrivier\n",
    "driver = webdriver.Chrome()\n",
    "driver.implicitly_wait(10)\n",
    "driver.set_script_timeout(120)\n",
    "driver.set_page_load_timeout(10)\n",
    "\n",
    "target_url = \"https://www.indeed.com\"\n",
    "\n",
    "driver.get(target_url) # go to indeed.com\n",
    "time.sleep(3)\n",
    "\n",
    "# search for data scientist\n",
    "input = driver.find_element(By.CSS_SELECTOR, '#text-input-what')\n",
    "time.sleep(5) # 5s\n",
    "input.send_keys('data scientist\\n')\n",
    "time.sleep(3) # 3s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502adba8",
   "metadata": {},
   "source": [
    "2. Save the result to local drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f798f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the resulting HTML\n",
    "page_html = driver.page_source #get page html\n",
    "\n",
    "# locate the path to store the html result\n",
    "save_location = os.getcwd() # get folder location\n",
    "print('HTML Pages are saved here:',save_location)\n",
    "\n",
    "# setup file name\n",
    "file_name = 'job_datascientists.html'\n",
    "file = os.path.join(save_location, file_name) \n",
    "\n",
    "# get page html\n",
    "page_html = driver.page_source\n",
    "\n",
    "# save file\n",
    "with open(file, 'w+',encoding='utf-8') as f:\n",
    "        f.write(page_html)\n",
    "        f.close()\n",
    "print(\"file saving completed!\")        \n",
    "\n",
    "time.sleep(3) # 3s\n",
    "\n",
    "# close the window\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c467067",
   "metadata": {},
   "source": [
    "3. Organize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4869c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the path to store the html result\n",
    "save_location = os.getcwd() # get folder location\n",
    "print('HTML Pages are saved here:',save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab8fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(save_location):\n",
    "    # Check if the file ends with .html\n",
    "    if filename.endswith(\".html\"):\n",
    "        # Construct the full file path\n",
    "        filepath = os.path.join(save_location, filename)\n",
    "        #print(filename)\n",
    "        # Read file to string\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            html = file.read()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            # define a regular expression for tags contains 'Data Scientist'\n",
    "            role_re = re.compile(r'Data .*')\n",
    "            info_re = re.compile(r'jobMetaDataGroup.*')\n",
    "            \n",
    "            # search for related info\n",
    "            titles = soup.find_all('a', attrs={\"aria-label\": role_re})\n",
    "            companies = soup.find_all('span', attrs={\"data-testid\": 'company-name'})\n",
    "            info = soup.find_all('div', attrs={\"class\": info_re})  \n",
    "\n",
    "                \n",
    "        # print results            \n",
    "        for i in range(len(titles)):\n",
    "            print('Job title: ' + titles[i].text)\n",
    "            print('Company: ' + companies[i].text)\n",
    "            #print(info[i].text)\n",
    "            if 'Pay information not provided' in info[i].text:\n",
    "                        print(info[i].text[:28])\n",
    "            for item in info[i]:\n",
    "                detail = item.find_all('div', attrs={\"data-testid\": \"attribute_snippet_testid\"})\n",
    "                for i in range(len(detail)):\n",
    "                    if detail[i] != None: # some info are not provided\n",
    "                        print(detail[i].text)\n",
    "                #details = None\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
